{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "912e6977-dbb2-4abc-a452-888d4512fcdb",
   "metadata": {},
   "source": [
    "**Notebook Status:** Work in Progress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e51edd4-a927-48af-8267-6ce9ea84956f",
   "metadata": {},
   "source": [
    "# Inverse Transform Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91cac1b-0ba0-4a3f-884f-75831fd25020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.stats import dirichlet\n",
    "from scipy.special import erf\n",
    "\n",
    "#np.random.seed(4250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ee9495-06f4-4a2d-ad0a-ac679ebbd068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_pdf(x, mu, sigma):\n",
    "    return np.exp(-0.5*((x-mu)/sigma)**2)/(sigma*np.sqrt(2*np.pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc44114-9348-4a3f-8be4-5015d0dfe1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(-10,10,500)\n",
    "fig = px.area(x=X, y=normal_pdf(X, 0, 1))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c017d004-9f6b-4391-9832-081ef83f331e",
   "metadata": {},
   "source": [
    "Using this primitive probability measure, we may construct a composite measure as a convex combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345849de-2830-4e13-b4de-83df05868a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gaussians = 5\n",
    "mu_range = (-4, 4)\n",
    "sigma_range = (0.25, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3cc3bd-304a-4742-81cb-9eb545c0a605",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = dirichlet([1]*n_gaussians)\n",
    "convex_combination = sampler.rvs()[0]\n",
    "rand_mus = np.random.uniform(*mu_range, n_gaussians)\n",
    "rand_sigmas = np.random.uniform(*sigma_range, n_gaussians)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59834a13-d7b2-4fe1-b4f1-2716b9ff2d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_mixture = lambda x: (convex_combination[None,:] @ np.array([normal_pdf(x, rand_mus[i], rand_sigmas[i]) for i in range(n_gaussians)])).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3513463-1ff6-44f6-aa7a-5512aff36f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_pdf = gaussian_mixture(X)\n",
    "fig = px.area(x=X, y=target_pdf)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f698a8-843b-4f44-a3f6-2dfe2a469dd8",
   "metadata": {},
   "source": [
    "Here we see the individual (scaled) Gaussian components that are summed together to form the above target distribution we will approximate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263d8526-4c4b-4439-acbd-c6b34bee5836",
   "metadata": {},
   "outputs": [],
   "source": [
    "components = dict(x=[], y=[], component=[])\n",
    "for i in range(n_gaussians+1):\n",
    "    components['x'].extend(X)\n",
    "    if i:\n",
    "        components['y'].extend(convex_combination[i-1]*normal_pdf(X, rand_mus[i-1], rand_sigmas[i-1]))\n",
    "        components['component'].extend([i]*len(X))\n",
    "    else:\n",
    "        components['y'].extend(target_pdf)\n",
    "        components['component'].extend(['mixture']*len(X))\n",
    "\n",
    "fig = px.line(components, x='x', y='y', color='component')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d22a8c-6caf-4e5a-91c7-5e10fb4d6784",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae99eac1-478c-463a-90f3-74d69b345684",
   "metadata": {},
   "source": [
    "Since we have access to our mixture parameters, we may integrate the PDF $f(x)$ to obtain the cumulative distribution function (CDF) $F(x)$ of our target distribution. \n",
    "\n",
    "$$F(x) = \\int_{-\\infty}^{\\infty}f(x)\\ dx.$$\n",
    "\n",
    "Substituting $f(x) = \\sum_{k=1}^{n}c_k\\cdot p(x;\\mu_k,\\sigma_k)$ and applying elementary integration rules we may rewrite the target mixture CDF as\n",
    "\n",
    "$$\\sum_{k=1}^{n}c_k\\cdot\\int_{-\\infty}^{\\infty}p(x;\\mu_k,\\sigma_k)\\ dx$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162f190e-5468-485b-8765-534b6b54dbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_cdf(x, mu, sigma):\n",
    "    return 0.5*(1+erf((x-mu)/(sigma*np.sqrt(2))))\n",
    "\n",
    "fig = px.line(x=X, y=normal_cdf(X, 0, 1))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63469b53-6cd7-4b49-a535-1144d1c19078",
   "metadata": {},
   "outputs": [],
   "source": [
    "mixture_cdf = lambda x: (convex_combination[None,:] @ np.array([normal_cdf(x, rand_mus[i], rand_sigmas[i]) for i in range(n_gaussians)])).ravel()\n",
    "target_cdf = mixture_cdf(X)\n",
    "fig = px.line(x=X, y=target_cdf)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61fe491-dce3-49c0-91d6-0bdcca30eefc",
   "metadata": {},
   "source": [
    "Inverting this new CDF is not as straightforward analytically (though possible by manipulating the Maclaurin series representation of $\\text{erf}^{-1}(x)$). For our purposes, we can approximate the inverse CDF by swapping the sampled x and y values of the target CDF and interpolating between these values. Given enough evenly-spaced samples of our target CDF, linear interpolation is sufficient and fast to compute compared to other interpolation methods, however we will consider other approaches later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d535b72a-7106-447a-9f9a-446995cd23bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "interval = np.linspace(0, 1, 1000)\n",
    "inverse_cdf = np.interp(interval, target_cdf, X)\n",
    "fig = px.line(x=interval, y=inverse_cdf)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583739a2-cc9a-4381-86a7-093e7f94b651",
   "metadata": {},
   "source": [
    "To sample from our target pdf, we implement inverse transform sampling.\n",
    "\n",
    "Note that we exclude samples at the boundary of our interval $[-10, 10]$ to minimize artifacting in our sampler. In short, these artifacts emerge from the clipping process: whereas our target PDF is defined over the entire real line, our sampler is only defined over a closed interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e06341b-283e-4d4d-90c4-de3d16852de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniform_samples = np.random.random(10000)\n",
    "target_samples = np.interp(uniform_samples, target_cdf, X)\n",
    "\n",
    "# Remove boundary samples\n",
    "target_samples = target_samples[np.abs(target_samples)<10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db53adec-3e74-451e-9782-16fe64df0bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=target_samples, histnorm='probability density', nbinsx=50, name='Histogram'))\n",
    "fig.add_trace(go.Scatter(x=X, y=target_pdf, mode='lines', name='True PDF', line={'dash': 'dash'}))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed6f7a7-a8ed-4843-9157-ba96d6066636",
   "metadata": {},
   "source": [
    "# Kernel Density Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69095941-43c0-4085-b951-ffd611f85b44",
   "metadata": {},
   "source": [
    "In the previous section, we showed how to randomly sample from a known target distribution using inverse transform sampling (ITS). Unfortunately, for data we collect in the real world, we may not know a priori what distribution best fits our observations. For many data modelling problems, there are justified theoretical assumptions we can make, such as choosing a normal distribution to describe the distribution of adult heights across the world population, or fitting an exponential distribution to model the elapsed time between receiving two consecutive support tickets (in the context of a help desk). Such examples fall into the realm of **parametric** statistics. More generally however, if we want to describe data with minimal assumptions, we must resort to applying **non-parametric** methods. Note that while the generalized applicability of non-parametric methods is desirable, such benefits often come with trade-offs such as increased computational costs or error, which we discuss further below. \n",
    "\n",
    "In the following sections, we explore one such non-parametric method known as **kernel density estimation** (KDE) to construct an approximation of some target distribution, then show how we may implement ITS to to sample from this more general set of probability measures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b298e4-04a8-48e0-8137-51bc1786a94c",
   "metadata": {},
   "source": [
    "### What is a \"Kernel\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ef8293-7ea7-465d-bb3d-c40be0bf289e",
   "metadata": {},
   "source": [
    "Although \"kernel\" refers to many different objects in mathematics, such as the kernel of a matrix (i.e., its nullspace), in the case of *kernel density estimation* this case refers to a \"smoothing\" function \n",
    "$K:X\\to\\mathbb{R}^{\\ge 0}$ from some set $X$ to the real interval $[0, \\infty)$. Intuitively, we can interpret such functions as assigning a non-negative weight to each element of $X$ Thus, probability measures may be understood as kernels whose total weight is normalized to sum (or integrate) to 1.\n",
    "\n",
    "Another usage of the name \"kernel\", especially in the context of machine learning, optimization, functional analysis, operator theory, and other related domains, refers to a generalization of positive (semi)definite functions and operators (for example, positive definite matrices).\n",
    "\n",
    "Let $X$ be a non-empty set. A symmetric binary function $K: X\\times X\\to\\mathbb{R}$ is a positive semidefinite kernel if the following properties hold:\n",
    "\n",
    "- For all $x,y\\in X$, we have that $K(x, y) = K(y, x)$ (*symmetry*),\n",
    "- For all $x_1\\dots x_n\\in X$, with $n$ a natural number, and all $a_1\\dots a_n\\in\\mathbb{R}$, we have that $\\sum_{i=1}^{n}\\sum_{j=1}^{n}a_i a_jK(x_i,x_j)\\ge 0$ (*positive semidefiniteness*).\n",
    "\n",
    "We say a kernel function is *positive definite* when the equality $\\sum_{i=1}^{n}\\sum_{j=1}^{n}a_i a_jK(x_i,x_j)=0$ implies that $a_i=0$ for all $i$.\n",
    "\n",
    "Although these two descriptions of kernels are seemingly unrelated, there exists a fundamental duality between probability measures and positive definite kernels, as formalized in Bochner's theorem. We will return to this connection in a following section; for now, we will focus on the former notion of a kernel as a positive measure in the context of nonparametric statistical analysis (skip section on *random Fourier features* for more details)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89430e43-4e4c-4075-9323-2dd9d2282c6d",
   "metadata": {},
   "source": [
    "### Examples of Kernels for Density Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ef0c6f-526c-46d1-96ad-4a574d660364",
   "metadata": {},
   "outputs": [],
   "source": [
    "interval = np.linspace(-1, 1, 100)\n",
    "compact_data = dict(\n",
    "    uniform = 0*interval + 0.5,\n",
    "    triangular = 1-np.abs(interval),\n",
    "    biweight = (15/16)*(1-interval**2)**2,\n",
    "    parabolic = 0.75*(1-interval**2),\n",
    ")\n",
    "\n",
    "fig = make_subplots(rows=1, cols=len(compact_data), shared_yaxes=True)\n",
    "for i, key in enumerate(compact_data.keys()):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=interval, y=compact_data[key], name=key, fill='tozeroy'),\n",
    "        row=1, col=i+1\n",
    "    )\n",
    "fig.update_layout(title_text=r\"Normalized Kernels with Compact Support: [-1, 1]\", yaxis_title=\"Density\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df628ff-0661-4258-b1d4-ce91133f5ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "interval = np.linspace(-5, 5, 100)\n",
    "inf_data = dict(\n",
    "    gaussian = (1/np.sqrt(2*np.pi))*np.exp(-0.5*interval**2),\n",
    "    sigmoid = 2/(np.pi*(np.exp(interval)+np.exp(-interval))),\n",
    ")\n",
    "\n",
    "fig = make_subplots(rows=1, cols=len(inf_data), shared_yaxes=True)\n",
    "for i, key in enumerate(inf_data.keys()):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=interval, y=inf_data[key], name=key, fill='tozeroy'),\n",
    "        row=1, col=i+1\n",
    "    )\n",
    "fig.update_layout(title_text=\"Normalized Kernels with Infinite Support: (-∞, ∞)\", yaxis_title='Density')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecc857d-590f-4607-8184-fc71cb516af8",
   "metadata": {},
   "source": [
    "### KDE vs. Histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713f952b-2f18-484b-bfa6-db19e51234c6",
   "metadata": {},
   "source": [
    "We may understand kernel density estimation as a generalization of histogram construction. To build a histogram, we first partition our sample space into a finite number of bins (i.e., compact connected disjoint subsets) that cover our space such that each sample we observe may be associated with exactly one bin. When observing data, we increment the weight of the bin associated with a particular sample, divided by the total number of samples in our dataset, if normalization is required. In other words, each data sample adds a unit of mass to a particular bin. At the end of this process, we are left with a discrete distribution that approximates the underlying distribution of our dataset. Since histograms are often applied to preliminary data visualization, their practical utility is typically limited to data of dimensions $n\\le 3$, though this process is valid in any dimension.\n",
    "\n",
    "The notion of a data sample adding \"mass\" to a histogram is preserved in kernel density estimation, however this mass is instead represented by a kernel centered at the data point. Intuitively, we want our mass (probability density) to concentrate around our observed data, regardless of where the data may lie in space. Thus, it is necessary to impose the additional condition of *translation-invariance* to our kernel so that $K(\\mathbf{x},\\mathbf{y})=K(\\mathbf{x}+\\mathbf{a},\\mathbf{y}+\\mathbf{a})$ for all\n",
    "$\\mathbf{x},\\mathbf{y},\\mathbf{a}\\in\\mathbb{R}^n$. To denote a translation-invariant kernel, we use the standard notation $K(\\mathbf{x}-\\mathbf{y})$ to emphasize that the kernel depends only on the *difference* between $\\mathbf{x}$ and $\\mathbf{y}$. Note that the examples of kernels provided in the previous section are all translation-invariant, and are thus appropriate for density estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e39e1d-4814-4248-8181-375e9b8b730b",
   "metadata": {},
   "source": [
    "### Computing a KDE from Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2eb052-a04b-43ef-9eb3-5820561ba533",
   "metadata": {},
   "source": [
    "Let $X\\subseteq\\mathbb{R}^d$ with $\\{x_1,\\dots,x_n\\}$ be a collection of independent and identically distributed (i.i.d.) samples drawn from from an unknown distribution $f$ over $X$, and let $K_h:X\\times X\\to\\mathbb{R}$ be a normalized translation-invariant kernel with *bandwidth* $h\\in\\mathbb{R}^{+}$. Then, the kernel density estimator of $f$ at a point $x\\in X$ is given by:\n",
    "$$ \n",
    "\\hat{f_h}(x) = \\frac{1}{nh^d}\\sum_{i=1}^{n}K(\\frac{x-x_i}{h}).\n",
    "$$\n",
    "Note that the bandwidth $h$ serves to scale the width of our chosen kernel, allowing for finer control over the \"spread\" of density around each sample, such that values of $h$ closer to $0$ result in a \"spikier\" result (i.e., there is more high frequency information) and larger values of $h$ tend towards a \"smoother\" (low frequency) estimate. Below we show how the choice of bandwidth affects the KDE of our target gaussian mixture from before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0800407f-687c-4961-ab22-bf48e3abca0c",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28be22a5-d4b8-4b55-b09e-273e0cc88f94",
   "metadata": {},
   "source": [
    "First, we define our kernel estimator to take as input a translation-invariant kernel, an array of i.i.d samples, an optional bandwidth parameter, and the point $x$ at which to estimate the unknown density. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b58f44c-9578-4372-bb6f-9a5915fce169",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KDE(kernel_function, samples, x, bandwidth=1):\n",
    "    scale = 1/(len(samples)*bandwidth)\n",
    "    res = 0\n",
    "    for sample in samples:\n",
    "        res += scale*kernel_function((x-sample)/bandwidth)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97460b0-d76f-4ac3-988b-ed89a54e8d25",
   "metadata": {},
   "source": [
    "Next we draw i.i.d. samples from our target distribution, simulating some ideal observation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d71685-872d-43c5-84d4-ed3f07957842",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_observations = 100\n",
    "samples = np.interp(np.random.random(n_observations), target_cdf, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3635f0-23fa-4f3e-89ff-b3e2904cfd9e",
   "metadata": {},
   "source": [
    "To illustrate the effect of various choices of bandwidth, we fix a particular kernel and dataset for our estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074cb18e-83ab-4f28-8e3c-dd3a69f89370",
   "metadata": {},
   "outputs": [],
   "source": [
    "parabolic_kernel = lambda x: np.array([0 if abs(v) > 1 else 0.75*(1-v**2) for v in x])\n",
    "gaussian_kernel = lambda x: (1/np.sqrt(2*np.pi))*np.exp(-0.5*x**2)\n",
    "\n",
    "bandwidths = [0.25, 0.5, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29eec85c-07fd-412f-8025-5beab6b1f122",
   "metadata": {},
   "outputs": [],
   "source": [
    "density_estimates = dict(\n",
    "    parabolic = {h: KDE(parabolic_kernel, samples, X, bandwidth=h) for h in bandwidths},\n",
    "    gaussian = {h: KDE(gaussian_kernel, samples, X, bandwidth=h) for h in bandwidths},\n",
    ")\n",
    "\n",
    "fig = make_subplots(rows=2, \n",
    "                    cols=len(bandwidths), \n",
    "                    shared_yaxes=True,\n",
    "                    subplot_titles=[f'BW: {bandwidths[i]}' for i in range(len(bandwidths))]\n",
    ")\n",
    "\n",
    "for i, kernel in enumerate(density_estimates.keys()):\n",
    "    for j, h in enumerate(density_estimates[kernel].keys()):\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=X, \n",
    "                y=density_estimates[kernel][h], \n",
    "                name=f\"Estimate ({kernel})\", \n",
    "                fill='tozeroy', \n",
    "                line={'color': ['blue', 'green'][i]},\n",
    "                legendgroup=0,\n",
    "                showlegend=not bool(j),\n",
    "            ), \n",
    "            row=i+1, col=j+1\n",
    "        )\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=X, \n",
    "                y=target_pdf, \n",
    "                name=\"True Density\",\n",
    "                line={'dash': 'dash', 'color': 'red'},\n",
    "                legendgroup=1,\n",
    "                showlegend=not bool(i+j),\n",
    "            ), \n",
    "        row=i+1, col=j+1\n",
    "        )\n",
    "\n",
    "fig.update_layout(title='KDE Result vs. Bandwidth', yaxis_title=\"Density\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981210b9-8dd9-450d-a8bc-d5c3dfcdb328",
   "metadata": {},
   "source": [
    "### Variable Bandwidth KDE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8576ebdc-4176-476a-a1c8-2c983292c624",
   "metadata": {},
   "source": [
    "See [Ref 1](https://journals.sagepub.com/doi/pdf/10.1177/1536867X0300300203) and [Ref 2](https://perso.univ-rennes2.fr/system/files/users/rouviere_l/hvariable-fin.pdf) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6efaf3-7822-44bf-ab9f-61030d7638df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VBKDE(kernel_function, samples, x, iters=1, bandwidth=1):\n",
    "    weights = np.ones(len(samples)) \n",
    "    scale = 1/(len(samples)*bandwidth)\n",
    "    for i in range(iters):\n",
    "        sample_density = 0\n",
    "        for j, val in enumerate(samples):\n",
    "            sample_density += (scale/weights[j])*kernel_function((samples-val)/(bandwidth*weights[j]))\n",
    "        #gmean = np.prod(sample_density)**(1.0/len(samples))  # unstable\n",
    "        gmean = np.exp(np.sum(np.log(np.abs(sample_density)))/len(samples)) \n",
    "        weights = (gmean/sample_density)**0.5\n",
    "    res = 0\n",
    "    for i, val in enumerate(samples):\n",
    "        res += (scale/weights[i])*kernel_function((x-val)/(bandwidth*weights[i]))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02e5243-b95b-495b-aea4-80bfa291727e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 5\n",
    "density_estimates = dict(\n",
    "    parabolic = {h: VBKDE(parabolic_kernel, samples, X, iters=n_iters, bandwidth=h) for h in bandwidths},\n",
    "    gaussian = {h: VBKDE(gaussian_kernel, samples, X, iters=n_iters, bandwidth=h) for h in bandwidths},\n",
    ")\n",
    "\n",
    "fig = make_subplots(rows=2, \n",
    "                    cols=len(bandwidths), \n",
    "                    shared_yaxes=True,\n",
    "                    subplot_titles=[f'BW: {bandwidths[i]}' for i in range(len(bandwidths))]\n",
    ")\n",
    "\n",
    "for i, kernel in enumerate(density_estimates.keys()):\n",
    "    for j, h in enumerate(density_estimates[kernel].keys()):\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=X, \n",
    "                y=density_estimates[kernel][h], \n",
    "                name=f\"Estimate ({kernel})\", \n",
    "                fill='tozeroy', \n",
    "                line={'color': ['blue', 'green'][i]},\n",
    "                legendgroup=0,\n",
    "                showlegend=not bool(j),\n",
    "            ), \n",
    "            row=i+1, col=j+1\n",
    "        )\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=X, \n",
    "                y=target_pdf, \n",
    "                name=\"True Density\",\n",
    "                line={'dash': 'dash', 'color': 'red'},\n",
    "                legendgroup=1,\n",
    "                showlegend=not bool(i+j),\n",
    "            ), \n",
    "        row=i+1, col=j+1\n",
    "        )\n",
    "\n",
    "fig.update_layout(title='Variable Bandwidth KDE (Sample Point) w/ Initial Bandwidth', yaxis_title=\"Density\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac104ce-6fb1-453e-9f61-38efee5b5b31",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61bcf64c-60a4-4799-a489-94852beb4662",
   "metadata": {},
   "source": [
    "# Monte Carlo Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104c4e9b-3634-4ec6-a68d-bd5819179e69",
   "metadata": {},
   "source": [
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfb586d-d02a-4f31-ad90-ce333f3205ad",
   "metadata": {},
   "source": [
    "# Random Fourier Features "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd66691-4bf4-4ff9-aca1-b6348cb379e3",
   "metadata": {},
   "source": [
    "Per Bochner's theorem: Suppose $f$ is a normalized positive definite kernel, where normalization means that $f(0)=1$. Then, there exists a unique probability distribution $p$ such that $f(x)=\\int_{-\\infty}^{\\infty}p(\\omega)e^{i\\omega x}\\ d\\omega$. In other words, $f$ and $p$ are related to each other by the Fourier transform.\n",
    "\n",
    "Rahimi and Recht ([ref](https://people.eecs.berkeley.edu/~brecht/papers/07.rah.rec.nips.pdf)) detail how we may construct an estimator of $f$ through the use of *random Fourier features* (RFF) -- this process reduces the computational cost of evaluating kernel functions, at the expense of accuracy (though error is inversely proportional to the number of features). Their approach is useful in the context of support vector machines, density estimation, and other kernel-based methods, especially when the explicit computation of a kernel function is expensive (more on this further below).\n",
    "\n",
    "In short, we can rewrite the expression $\\int_{-\\infty}^{\\infty}p(\\omega)e^{i\\omega x}\\ d\\omega$ using the law of the unconscious statistician as the expectation $\\mathbb{E}_\\omega[e^{i\\omega x}]$. Note that this is also the definition of a *characteristic function* of a probability distribution. Then, an unbiased estimator of $f$ may be constructed via Monte Carlo integration:\n",
    "\n",
    "$$f(x)=\\mathbb{E}_\\omega[e^{i\\omega x}]\\approx\\frac{1}{N}\\sum_{k=1}^{N}e^{i\\omega x},$$\n",
    "\n",
    "where $N$ is the number of samples $\\omega$ drawn uniformly from $p$. Because $e^{ix}$ is a complex-valued function, when working with real kernels we can instead use the estimator $\\frac{1}{N}\\sum_{k=1}^{N}\\cos({\\omega x}),$ since $e^{ix}=\\cos(x)+i\\sin(x)$, per Euler. \n",
    "\n",
    "Below we show some examples of univariate kernel functions over $\\mathbb{R}$ estimated using this method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56fecf3-0ccc-4c79-bf82-6dccfecaaedf",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872185cf-ec84-4ce5-bdd9-9743ba5934d4",
   "metadata": {},
   "source": [
    "Use the sliders in the following examples to see how each estimator converges to the true kernel function as the number of samples increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6d0510-ae84-4b43-aece-e2c89aaed0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define simulation variables\n",
    "max_samples = 2000\n",
    "srange = np.arange(0, max_samples+1, 5)\n",
    "srange[0] = 1\n",
    "n_frames = len(srange)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77933da-4b5f-44ef-aaa5-38261044e1d7",
   "metadata": {},
   "source": [
    "#### Example 1: Gaussian Distribution\n",
    "Here we sample from the standard Gaussian distribution, whose Fourier transform is another Gaussian function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff55d951-00c3-4721-a766-eb2cca4a3dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from standard Gaussian\n",
    "samples = np.random.normal(0, 1, max_samples)\n",
    "\n",
    "fig = go.Figure()\n",
    "for f in range(n_frames):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x = X,\n",
    "            y = (1/len(samples[:srange[f]])) * np.sum(np.cos(np.outer(samples[:srange[f]], X)), axis=0),\n",
    "            name = 'estimate',\n",
    "            fill='tozeroy',\n",
    "            visible = False if f else True,\n",
    "        )\n",
    "    )\n",
    "\n",
    "steps = []\n",
    "for i in range(n_frames):\n",
    "    step = dict(\n",
    "        method=\"restyle\",\n",
    "        args=[{\"visible\": [False] * n_frames + [True]}],\n",
    "        label=str(srange[i]),\n",
    "    )\n",
    "    step[\"args\"][0][\"visible\"][i] = True\n",
    "    steps.append(step)\n",
    "\n",
    "sliders = [dict(\n",
    "    active=0,\n",
    "    currentvalue={\"prefix\": \"Num. Samples: \"},\n",
    "    pad={\"t\": 50},\n",
    "    steps=steps\n",
    ")]\n",
    "\n",
    "fig.update_layout(\n",
    "    sliders=sliders,\n",
    "    title='Gaussian Kernel Approximation',\n",
    "    yaxis=dict(title='y'),\n",
    "    xaxis=dict(title='x'),\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Scatter(x=X, y=np.exp(-0.5*X**2), line={'dash':'dash'}, name='true kernel'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2c277c-f712-45a5-85ab-0815a1a9c02e",
   "metadata": {},
   "source": [
    "#### Example 2: Uniform Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc1523f-a24e-411c-b578-66a24dc57009",
   "metadata": {},
   "source": [
    "Here we sample from a uniform distribution over the interval $[-\\pi,\\pi]$, whose Fourier transform is $\\operatorname{sinc}(x)=\\frac{\\sin({\\pi x})}{\\pi x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e298287e-3387-490f-97ee-b1d80f5ba92a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sample from uniform distribution over circle\n",
    "samples = np.random.uniform(-np.pi, np.pi, max_samples)\n",
    "\n",
    "fig = go.Figure()\n",
    "for f in range(n_frames):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x = X,\n",
    "            y = (1/len(samples[:srange[f]])) * np.sum(np.cos(np.outer(samples[:srange[f]], X)), axis=0),\n",
    "            name = 'estimate',\n",
    "            fill='tozeroy',\n",
    "            visible = False if f else True,\n",
    "        )\n",
    "    )\n",
    "\n",
    "steps = []\n",
    "for i in range(n_frames):\n",
    "    step = dict(\n",
    "        method=\"restyle\",\n",
    "        args=[{\"visible\": [False] * n_frames + [True]}],\n",
    "        label=str(srange[i]),\n",
    "    )\n",
    "    step[\"args\"][0][\"visible\"][i] = True\n",
    "    steps.append(step)\n",
    "\n",
    "sliders = [dict(\n",
    "    active=0,\n",
    "    currentvalue={\"prefix\": \"Num. Samples: \"},\n",
    "    pad={\"t\": 50},\n",
    "    steps=steps\n",
    ")]\n",
    "\n",
    "fig.update_layout(\n",
    "    sliders=sliders,\n",
    "    title='Sinc Kernel Approximation',\n",
    "    yaxis=dict(title='y'),\n",
    "    xaxis=dict(title='x'),\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Scatter(x=X, y=np.sin(np.pi*X)/(np.pi*X), line={'dash':'dash'}, name='true function'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4041120-4758-4715-8f02-ab5249e5d4d0",
   "metadata": {},
   "source": [
    "#### Example 3: Parabolic Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87afe51-5e10-4c73-bc77-65057f8bd188",
   "metadata": {},
   "source": [
    "Here we sample from a parabolic distribution using ITS and apply the same procedure as above. Note that the Fourier transform of this distribution does not have a standard name (as far as I am aware), though it can be solved for in closed form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30a28e2-75b9-4643-8106-1246e02fc8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from normalized parabolic distribution using inverse transform sampling.\n",
    "unit = np.linspace(-1, 1, 200)\n",
    "cdf = -0.25*(unit**3-3*unit-2)\n",
    "ppf = lambda x: np.interp(x, cdf, unit)\n",
    "rand = np.random.random(max_samples)\n",
    "samples = ppf(rand)\n",
    "\n",
    "fig = go.Figure()\n",
    "for f in range(n_frames):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x = X,\n",
    "            y = (1/len(samples[:srange[f]])) * \\\n",
    "                np.sum(np.cos(np.outer(np.pi*samples[:srange[f]], X)), axis=0),  # (accounts for pi stretch factor)\n",
    "            name = 'estimate',\n",
    "            fill='tozeroy',\n",
    "            visible = False if f else True,\n",
    "        )\n",
    "    )\n",
    "\n",
    "steps = []\n",
    "for i in range(n_frames):\n",
    "    step = dict(\n",
    "        method=\"restyle\",\n",
    "        args=[{\"visible\": [False] * n_frames + [True]}],\n",
    "        label=str(srange[i]),\n",
    "    )\n",
    "    step[\"args\"][0][\"visible\"][i] = True\n",
    "    steps.append(step)\n",
    "\n",
    "sliders = [dict(\n",
    "    active=0,\n",
    "    currentvalue={\"prefix\": \"Num. Samples: \"},\n",
    "    pad={\"t\": 50},\n",
    "    steps=steps\n",
    ")]\n",
    "\n",
    "fig.update_layout(\n",
    "    sliders=sliders,\n",
    "    title='Parabolic Kernel Approximation',\n",
    "    yaxis=dict(title='y'),\n",
    "    xaxis=dict(title='x'),\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Scatter(x=X, y=3*(np.sin(np.pi*X)-np.pi*X*np.cos(np.pi*X))/(np.pi*X)**3, line={'dash':'dash'}, name='true function'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e534665-a65d-459e-af72-efca738b4279",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ffd4654-8b5a-492e-a65c-20a41909cdd6",
   "metadata": {},
   "source": [
    "# TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17e31ea-34d9-43cd-8e68-6966947627a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
